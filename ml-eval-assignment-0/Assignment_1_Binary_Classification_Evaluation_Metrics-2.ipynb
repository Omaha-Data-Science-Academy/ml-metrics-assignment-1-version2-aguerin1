{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d37a6bc0",
      "metadata": {
        "id": "d37a6bc0"
      },
      "source": [
        "# Assignment 1 - Binary Classification Evaluation Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1679a82",
      "metadata": {
        "id": "d1679a82"
      },
      "source": [
        "**Objective:**\n",
        "The objective of this assignment is to assess your understanding of fundamental concepts in model evaluation for machine learning tasks. This assignment covers topics discussed in the first half of the course, including key evaluation metrics, confusion matrices, ROC curves, and Precision-Recall curves.\n",
        "Instructions:\n",
        "\n",
        "1. Theory Questions:\n",
        "Answer the following theoretical questions:\n",
        "\n",
        "    1. Explain the limitations of accuracy as an evaluation metric in imbalanced datasets. How does accuracy behave when classes are heavily skewed, and why might it provide misleading results?\n",
        "    2. Describe the purpose and interpretation of a confusion matrix. How does it help in assessing a classification model's performance?\n",
        "    3. Explain the concept of ROC curves. What does each point on an ROC curve represent? How is the area under the ROC curve (AUC-ROC) calculated?\n",
        "    4. Compare and contrast the advantages and disadvantages of ROC curves and Precision-Recall curves. In what scenarios would you prefer to use one over the other, and why?\n",
        "\n",
        "2. Practical Exercises:\n",
        "* Implement Python code to calculate the following evaluation metrics for a given binary classification problem: Log Loss\n",
        "* Select the best metric for an applied scenario\n",
        "\n",
        "**Submission Guidelines:**\n",
        "* Submit your responses to the theory questions in a neatly organized markdown.\n",
        "* Include your Python code for the practical exercise.\n",
        "* Submit your assignment as a single `.ipynb` file named `MY NAME Assignment 1 - Log Loss` via the course submission platform (slack)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e58864fe",
      "metadata": {
        "id": "e58864fe"
      },
      "source": [
        "## Part 1: Theory Questions (20 points)\n",
        "Provide your answers here:\n",
        "\n",
        "    1.  Accuracy has many limitations as an evaluation metric for an imbalanced datasets. Accuracy is measured by the proportion of correctly classified instances out of the total number of instances. However, with an imbalanced data set, it may have 99 negative cases and 1 positive case. With accuracy as the metric, the model could predict only negative cases and have a 99% accuracy while still never accounting for any positive cases. This can lead to a high accuracy while the model is unable to provide information on when a positive case could occur.\n",
        "    2.  The purpose of a confusion matrix is to describe the performance of a classification model on a set of test data for which the true values are known. It helps to visualize the differences between the predicted and actual values for each of the different classes. Using the confusion matrix, it is also easy to calculate a variety of different metrics such as Sensitivity, Specificity, Accuracy, and Precision and each of those can be used to quantify a model's performance.\n",
        "    3.  Receiver and Operator Curve (ROC) is a graph which helps visualize how the performance changes as the threshold changes by plotting the TPR against the FPR. Each point on an ROC curve represents a different threshold used for classifying a positive or negative outcome. The area under the ROC curve is calculated by taking the integral of the ROC (mathematical method) or using the roc_auc_score to determine the area under the curve.\n",
        "    4. There are many advantages and disadvantages to ROC and Precision-Recall curves. The advantages of ROC is it provides a comprehensive perspective of the model as it compares performance over all thresholds, has a balanced evaluation, and provides a single metric summary. The disadvantage of ROC is it does not handle imbalanced data well as the true negative rate can have a high impact. The advantages of PR curve is that it focuses on the positive class and can be very informative with imbalanced data set as it does not include true negative rate. The disadvantage of PR curve is that it is a less comprehensive summary of the data and there is no single metric for it. Based on these advantages and disadvantages, ROC is more suited to balanced datasets requiring an overall picture, while PR curve is more suited to imbalanced datasets where the focus is a minority class."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b8c2adb",
      "metadata": {
        "id": "8b8c2adb"
      },
      "source": [
        "## Practicing Log Loss (25 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f4dca41",
      "metadata": {
        "id": "2f4dca41"
      },
      "source": [
        "**Objective:**\n",
        "The objective of this assignment is to deepen your understanding of log loss, also known as logarithmic loss or cross-entropy loss, and its application in evaluating the performance of classification models.\n",
        "\n",
        "**Instructions:**\n",
        "In this assignment, you will be given a set of binary classification predictions along with their corresponding actual class labels. Your task is to calculate the log loss for each prediction and then analyze the overall log loss performance of the model.\n",
        "\n",
        "**Dataset:**\n",
        "You are provided with a dataset containing the following information:\n",
        "\n",
        "Predicted probabilities for the positive class (ranging from 0 to 1) for a set of instances.\n",
        "Actual binary class labels (0 or 1) indicating whether the instance belongs to the positive class or not.\n",
        "\n",
        "**Assignment Tasks:**\n",
        "1. Calculate the log loss for each instance in the dataset using the predicted probabilities and actual class labels.\n",
        "2. Summarize the individual log losses and compute the overall log loss performance for the model.\n",
        "3. Interpret the overall log loss value and analyze the model's performance. Discuss any insights or observations derived from the log loss analysis.\n",
        "\n",
        "\n",
        "**Dataset:**\n",
        "\n",
        "| Instance | Predicted Probability | Actual Label |\n",
        "|----------|------------------------|--------------|\n",
        "|    1     |          0.9           |       1      |\n",
        "|    2     |          0.3           |       0      |\n",
        "|    3     |          0.6           |       1      |\n",
        "|    4     |          0.8           |       0      |\n",
        "|    5     |          0.1           |       1      |\n",
        "\n",
        "\n",
        "**Grading Criteria:**\n",
        "\n",
        "* Correctness of log loss calculations.\n",
        "* Clarity and completeness of the analysis.\n",
        "* Insights derived from the log loss interpretation.\n",
        "* Overall presentation and adherence to submission guidelines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3dad6832",
      "metadata": {
        "id": "3dad6832",
        "outputId": "a9a88ba1-bb30-411a-bba6-3a9b2c79fed8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Instance  Predicted Probability  Actual Label\n",
            "0         1                    0.9             1\n",
            "1         2                    0.3             0\n",
            "2         3                    0.6             1\n",
            "3         4                    0.8             0\n",
            "4         5                    0.1             1\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame with the dataset\n",
        "data = {\n",
        "    'Instance': [1, 2, 3, 4, 5],\n",
        "    'Predicted Probability': [0.9, 0.3, 0.6, 0.8, 0.1],\n",
        "    'Actual Label': [1, 0, 1, 0, 1]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "9be2a3c3",
      "metadata": {
        "id": "9be2a3c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8af4eb07-8182-4ef3-9e0c-e71c289dbe12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.976976817758139\n",
            "0.976976817758139\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "from sklearn.metrics import log_loss\n",
        "import numpy as np\n",
        "\n",
        "# Log Loss Function\n",
        "def logLoss(y_true, y_pred):\n",
        "    sum = y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)\n",
        "    return -np.mean(sum)\n",
        "\n",
        "# Test Log Loss Function\n",
        "logLoss = logLoss(df['Actual Label'], df['Predicted Probability'])\n",
        "print(logLoss)\n",
        "\n",
        "# Verify Log Loss Function Answer\n",
        "logLoss = log_loss(df['Actual Label'], df['Predicted Probability'])\n",
        "print(logLoss)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eaa92db1",
      "metadata": {
        "id": "eaa92db1"
      },
      "source": [
        "*Question: Interpret the log loss above. How would it change if the predicted probability for instance 0 changed from 0.9 to 0.6? Why?*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2abec414",
      "metadata": {
        "id": "2abec414"
      },
      "source": [
        "*Your answer: The log loss above is not a very good log loss as the best log loss would be a log loss of zero. The log loss is not extremely bad as it is still less than 1, however, it is not good. If the instance 0 predicted probability changed from 0.9 to 0.6 then the log loss would increase showing a decrease in performance. This change would occur as it shows the model moved further in its prediction from the actual value.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "972c7485",
      "metadata": {
        "id": "972c7485"
      },
      "source": [
        "*Question: Why might you select log loss over precision, recall, or accuracy (in the context of any problem, not this one specifically)?*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88b26ee2",
      "metadata": {
        "id": "88b26ee2"
      },
      "source": [
        "*Your answer: The reason why log loss may be selected over other metrics is due to it accounting for both the correctness of the model's decision as well as the model's confidence in the decision. If the model is very confident in a wrong prediction, it is heavily penalized by the log loss algorithm.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a69ed6d7",
      "metadata": {
        "id": "a69ed6d7"
      },
      "source": [
        "## Application Scenario: Select a Metric (55 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "791158c2",
      "metadata": {
        "id": "791158c2"
      },
      "source": [
        "**Application Scenario: Fraud Detection System**\n",
        "\n",
        "You are working as a data scientist for a financial institution that wants to develop a fraud detection system to identify potentially fraudulent transactions. The dataset contains information about various transactions, including transaction amount, merchant ID, and transaction type. Your task is to build a machine learning model to classify transactions as either fraudulent or non-fraudulent.\n",
        "\n",
        "**Problem Description:**\n",
        "\n",
        "* Dataset: The dataset consists of historical transaction data, with labels indicating whether each transaction was fraudulent or not.\n",
        "* Class Distribution: The dataset is mostly non-fraudulant cases, with a small percentage of transactions being fraudulent compared to legitimate transactions.\n",
        "* Objective: The objective is to develop a fraud detection model that minimizes false negatives (fraudulent transactions incorrectly classified as non-fraudulent) while maintaining a reasonable level of precision.\n",
        "\n",
        "**Stakeholder Requirements:**\n",
        "Given the nature of the problem, it is crucial to prioritize recall (sensitivity) to ensure that as many fraudulent transactions as possible are detected. However, precision is also important to minimize false positives and avoid unnecessary investigations of legitimate transactions. Minimizing false negatives (missing fraudulent transactions) is of utmost importance.\n",
        "\n",
        "**Task:**\n",
        "Your task is to develop Python code to evaluate the performance of different machine learning models using various evaluation metrics, including accuracy, precision, recall, and F2 score. *Select the evaluation metric that best suits the problem and explain your choice*.\n",
        "\n",
        "**Additional Guidelines:**\n",
        "* You should preprocess the dataset as needed and split it into training and testing sets.\n",
        "* Implement machine learning models of your choice (e.g., logistic regression, random forest) and evaluate their performance.\n",
        "* Use appropriate evaluation metrics for binary classification tasks.\n",
        "* Discuss the rationale behind your choice of evaluation metric and how it aligns with the problem requirements.\n",
        "* Present your findings and recommendations for selecting the best model based on the chosen evaluation metric."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc4f0e46",
      "metadata": {
        "id": "dc4f0e46"
      },
      "source": [
        "**Dataset Sample:**\n",
        "\n",
        "| Transaction ID | Transaction Amount | Merchant ID | Transaction Type | Fraudulent |\n",
        "|----------------|--------------------|-------------|------------------|------------|\n",
        "| 1              | 1000               | M123        | Online Purchase  | 0          |\n",
        "| 2              | 500                | M456        | ATM Withdrawal   | 0          |\n",
        "| 3              | 2000               | M789        | Online Purchase  | 1          |\n",
        "| 4              | 1500               | M123        | POS Transaction  | 0          |\n",
        "| 5              | 800                | M456        | Online Purchase  | 0          |\n",
        "| 6              | 3000               | M789        | ATM Withdrawal   | 1          |\n",
        "\n",
        "* Transaction ID: Unique identifier for each transaction.\n",
        "* Transaction Amount: The amount of money involved in the transaction.\n",
        "* Merchant ID: Identifier for the merchant involved in the transaction.\n",
        "* Transaction Type: The type of transaction (e.g., online purchase, ATM withdrawal, POS transaction).\n",
        "* Fraudulent: Binary indicator (0 or 1) specifying whether the transaction is fraudulent (1) or not (0)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bc1ec81",
      "metadata": {
        "id": "1bc1ec81",
        "outputId": "215add05-0d1f-42e9-adc3-d011206a843a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Transaction ID  Transaction Amount Merchant ID Transaction Type  \\\n",
            "0                1                1000        M123  Online Purchase   \n",
            "1                2                 500        M456   ATM Withdrawal   \n",
            "2                3                2000        M789  Online Purchase   \n",
            "3                4                1500        M123  POS Transaction   \n",
            "4                5                 800        M456  Online Purchase   \n",
            "5                6                3000        M789   ATM Withdrawal   \n",
            "6                7                1200        M123  Online Purchase   \n",
            "7                8                 700        M456   ATM Withdrawal   \n",
            "8                9                1800        M789  Online Purchase   \n",
            "9               10                1300        M123  POS Transaction   \n",
            "10              11                 900        M456  Online Purchase   \n",
            "11              12                 400        M789   ATM Withdrawal   \n",
            "12              13                2200        M123  Online Purchase   \n",
            "13              14                1600        M456   ATM Withdrawal   \n",
            "14              15                 850        M789  Online Purchase   \n",
            "15              16                2800        M123  POS Transaction   \n",
            "16              17                1100        M456  Online Purchase   \n",
            "17              18                 600        M789   ATM Withdrawal   \n",
            "18              19                1900        M123  Online Purchase   \n",
            "19              20                1400        M456   ATM Withdrawal   \n",
            "20              21                 950        M123  Online Purchase   \n",
            "21              22                 300        M456   ATM Withdrawal   \n",
            "22              23                2100        M789  Online Purchase   \n",
            "23              24                1700        M123  POS Transaction   \n",
            "24              25                 820        M456  Online Purchase   \n",
            "25              26                3200        M789   ATM Withdrawal   \n",
            "26              27                1250        M123  Online Purchase   \n",
            "27              28                 720        M456   ATM Withdrawal   \n",
            "28              29                1850        M789  Online Purchase   \n",
            "29              30                1350        M123  POS Transaction   \n",
            "30              31                 880        M456  Online Purchase   \n",
            "31              32                 420        M789   ATM Withdrawal   \n",
            "32              33                2400        M123  Online Purchase   \n",
            "33              34                1750        M456   ATM Withdrawal   \n",
            "34              35                 830        M789  Online Purchase   \n",
            "35              36                3100        M123  POS Transaction   \n",
            "36              37                1150        M456  Online Purchase   \n",
            "37              38                 620        M789   ATM Withdrawal   \n",
            "38              39                1950        M123  Online Purchase   \n",
            "39              40                1450        M456   ATM Withdrawal   \n",
            "\n",
            "    Fraudulent  \n",
            "0            0  \n",
            "1            0  \n",
            "2            1  \n",
            "3            0  \n",
            "4            0  \n",
            "5            1  \n",
            "6            0  \n",
            "7            0  \n",
            "8            1  \n",
            "9            0  \n",
            "10           0  \n",
            "11           1  \n",
            "12           0  \n",
            "13           0  \n",
            "14           1  \n",
            "15           0  \n",
            "16           0  \n",
            "17           1  \n",
            "18           0  \n",
            "19           0  \n",
            "20           1  \n",
            "21           0  \n",
            "22           0  \n",
            "23           1  \n",
            "24           0  \n",
            "25           0  \n",
            "26           1  \n",
            "27           0  \n",
            "28           0  \n",
            "29           1  \n",
            "30           0  \n",
            "31           0  \n",
            "32           1  \n",
            "33           0  \n",
            "34           0  \n",
            "35           1  \n",
            "36           0  \n",
            "37           0  \n",
            "38           1  \n",
            "39           0  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Creating the dataset\n",
        "data = {\n",
        "    'Transaction ID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10,\n",
        "                       11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n",
        "                       21, 22, 23, 24, 25, 26, 27, 28, 29, 30,\n",
        "                       31, 32, 33, 34, 35, 36, 37, 38, 39, 40],\n",
        "    'Transaction Amount': [1000, 500, 2000, 1500, 800, 3000, 1200, 700, 1800, 1300,\n",
        "                           900, 400, 2200, 1600, 850, 2800, 1100, 600, 1900, 1400,\n",
        "                           950, 300, 2100, 1700, 820, 3200, 1250, 720, 1850, 1350,\n",
        "                           880, 420, 2400, 1750, 830, 3100, 1150, 620, 1950, 1450],\n",
        "    'Merchant ID': ['M123', 'M456', 'M789', 'M123', 'M456', 'M789', 'M123', 'M456', 'M789', 'M123',\n",
        "                    'M456', 'M789', 'M123', 'M456', 'M789', 'M123', 'M456', 'M789', 'M123', 'M456',\n",
        "                    'M123', 'M456', 'M789', 'M123', 'M456', 'M789', 'M123', 'M456', 'M789', 'M123',\n",
        "                    'M456', 'M789', 'M123', 'M456', 'M789', 'M123', 'M456', 'M789', 'M123', 'M456'],\n",
        "    'Transaction Type': ['Online Purchase', 'ATM Withdrawal', 'Online Purchase', 'POS Transaction', 'Online Purchase',\n",
        "                         'ATM Withdrawal', 'Online Purchase', 'ATM Withdrawal', 'Online Purchase', 'POS Transaction',\n",
        "                         'Online Purchase', 'ATM Withdrawal', 'Online Purchase', 'ATM Withdrawal', 'Online Purchase',\n",
        "                         'POS Transaction', 'Online Purchase', 'ATM Withdrawal', 'Online Purchase', 'ATM Withdrawal',\n",
        "                         'Online Purchase', 'ATM Withdrawal', 'Online Purchase', 'POS Transaction', 'Online Purchase',\n",
        "                         'ATM Withdrawal', 'Online Purchase', 'ATM Withdrawal', 'Online Purchase', 'POS Transaction',\n",
        "                         'Online Purchase', 'ATM Withdrawal', 'Online Purchase', 'ATM Withdrawal', 'Online Purchase',\n",
        "                         'POS Transaction', 'Online Purchase', 'ATM Withdrawal', 'Online Purchase', 'ATM Withdrawal'],\n",
        "    'Fraudulent': [0, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n",
        "                   0, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
        "                   1, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
        "                   0, 0, 1, 0, 0, 1, 0, 0, 1, 0]\n",
        "}\n",
        "\n",
        "# Creating DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Displaying the DataFrame\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAndHDyt6AP_",
        "outputId": "325acbb7-b6d7-4838-ee05-88ddfbf6f3fc"
      },
      "id": "eAndHDyt6AP_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 40 entries, 0 to 39\n",
            "Data columns (total 5 columns):\n",
            " #   Column              Non-Null Count  Dtype \n",
            "---  ------              --------------  ----- \n",
            " 0   Transaction ID      40 non-null     int64 \n",
            " 1   Transaction Amount  40 non-null     int64 \n",
            " 2   Merchant ID         40 non-null     object\n",
            " 3   Transaction Type    40 non-null     object\n",
            " 4   Fraudulent          40 non-null     int64 \n",
            "dtypes: int64(3), object(2)\n",
            "memory usage: 1.7+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "9G1rAFf-6USP"
      },
      "id": "9G1rAFf-6USP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Test Split Data\n",
        "X = df[['Transaction ID', 'Transaction Amount', 'Merchant ID', 'Transaction Type']]\n",
        "y = df['Fraudulent']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=43)"
      ],
      "metadata": {
        "id": "EKtsOy9h5SZu"
      },
      "id": "EKtsOy9h5SZu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert categorical variables into numeric using one-hot encoding\n",
        "X_train = pd.get_dummies(X_train, columns=['Merchant ID', 'Transaction Type'], drop_first=True)\n",
        "X_test = pd.get_dummies(X_test, columns=['Merchant ID', 'Transaction Type'], drop_first=True)"
      ],
      "metadata": {
        "id": "S9WHQFRj6p1r"
      },
      "id": "S9WHQFRj6p1r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameters to tune in the Logistic Regression model\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100, 250, 500],\n",
        "    'solver': ['liblinear', 'newton-cg', 'newton-cholesky'],\n",
        "    'max_iter': [100, 200, 300, 500]\n",
        "}\n",
        "\n",
        "# Apply GridSearchCV\n",
        "grid_search = GridSearchCV(LogisticRegression(),\n",
        "                           param_grid,\n",
        "                           cv=5,\n",
        "                           scoring='f1_macro')\n",
        "\n",
        "# Fit the model with GridSearch\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best score and parameters found by Grid Search\n",
        "print(\"Best score found: \", grid_search.best_score_)\n",
        "print(\"Best parameters found: \", grid_search.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSm48JUa7Y6V",
        "outputId": "fdcd45a5-4fbb-4e56-b2dc-5e2db528a701"
      },
      "id": "VSm48JUa7Y6V",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best score found:  0.5694444444444443\n",
            "Best parameters found:  {'C': 10, 'max_iter': 100, 'solver': 'newton-cg'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the test set using the best found parameters\n",
        "y_pred = grid_search.predict(X_test)"
      ],
      "metadata": {
        "id": "AXAvXbrU_YWz"
      },
      "id": "AXAvXbrU_YWz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model performance\n",
        "precision = precision_score(y_test, y_pred, average='micro')\n",
        "print(f\"Test Set Precision with Grid Search: {precision}\")\n",
        "\n",
        "recall = recall_score(y_test, y_pred, average='micro')\n",
        "print(f\"Test Set Recall with Grid Search: {recall}\")\n",
        "\n",
        "f1 = f1_score(y_test, y_pred, average='micro')\n",
        "print(f\"Test Set F1 with Grid Search: {f1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAqckC3J_XXb",
        "outputId": "1e75cab3-41c7-4d83-bb58-299d2f4c5857"
      },
      "id": "TAqckC3J_XXb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Set Precision with Grid Search: 0.5\n",
            "Test Set Recall with Grid Search: 0.5\n",
            "Test Set F1 with Grid Search: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification Report\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\\n\", report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3h4PMN59Ad4H",
        "outputId": "9aa4c7b1-5008-4894-9620-e61bafe2cf06"
      },
      "id": "3h4PMN59Ad4H",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.67      0.67         9\n",
            "           1       0.00      0.00      0.00         3\n",
            "\n",
            "    accuracy                           0.50        12\n",
            "   macro avg       0.33      0.33      0.33        12\n",
            "weighted avg       0.50      0.50      0.50        12\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose to use f1_macro as the scoring algorithm to optimize as f1 emphasizes both precision and recall. As the goal is to create a fraudulent detection algorithm which minimizes false negatives (recall) while still maintaining high precision. However, f1 has multiple options: micro, macro, and weighted. I specifically chose f1_macro as macro is a good choice when there is a class imbalance. In this scenario, a negative occurence (fraud) is much less likely to occur than a positive occurence (not fraud) which means that it is important that the algorithm optimizes both classes."
      ],
      "metadata": {
        "id": "HjUf0Y1FCQud"
      },
      "id": "HjUf0Y1FCQud"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}